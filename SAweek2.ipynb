{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6ggWRW3KE4rfysRDTGzVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashokmaster83/SummerAnalytics1/blob/main/SAweek2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.stats import mode\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"/content/hacktrain.csv\")\n",
        "test_df = pd.read_csv(\"/content/hacktest.csv\")\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "print(\"Class distribution:\")\n",
        "print(train_df['class'].value_counts())\n",
        "\n",
        "# Extract NDVI columns\n",
        "ndvi_columns = [col for col in train_df.columns if '_N' in col]\n",
        "print(f\"Found {len(ndvi_columns)} NDVI features\")\n",
        "\n",
        "X = train_df[ndvi_columns]\n",
        "y = train_df['class']\n",
        "X_test = test_df[ndvi_columns]\n",
        "ids_test = test_df['ID']\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"Missing values in train: {X.isnull().sum().sum()}\")\n",
        "print(f\"Missing values in test: {X_test.isnull().sum().sum()}\")\n",
        "\n",
        "# Advanced imputation with multiple strategies\n",
        "def advanced_imputation(X_train, X_test):\n",
        "    \"\"\"Apply multiple imputation strategies and combine them\"\"\"\n",
        "    # Strategy 1: KNN Imputation\n",
        "    knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
        "    X_train_knn = knn_imputer.fit_transform(X_train)\n",
        "    X_test_knn = knn_imputer.transform(X_test)\n",
        "\n",
        "    # Strategy 2: Forward/Backward fill (temporal approach)\n",
        "    X_train_temp = X_train.copy()\n",
        "    X_test_temp = X_test.copy()\n",
        "\n",
        "    # Forward fill then backward fill\n",
        "    X_train_temp = X_train_temp.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
        "    X_test_temp = X_test_temp.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
        "\n",
        "    # Combine strategies (weighted average)\n",
        "    X_train_combined = 0.8 * X_train_knn + 0.2 * X_train_temp.values\n",
        "    X_test_combined = 0.8 * X_test_knn + 0.2 * X_test_temp.values\n",
        "\n",
        "    return X_train_combined, X_test_combined\n",
        "\n",
        "# Apply advanced imputation\n",
        "X_imputed, X_test_imputed = advanced_imputation(X, X_test)\n",
        "\n",
        "# Apply noise reduction using Savitzky-Golay filter\n",
        "def denoise_time_series(X_data):\n",
        "    \"\"\"Apply Savitzky-Golay filter to reduce noise\"\"\"\n",
        "    X_denoised = np.zeros_like(X_data)\n",
        "    for i in range(X_data.shape[0]):\n",
        "        try:\n",
        "            # Apply smoothing filter\n",
        "            X_denoised[i] = savgol_filter(X_data[i], window_length=5, polyorder=2)\n",
        "        except:\n",
        "            # If smoothing fails, use original data\n",
        "            X_denoised[i] = X_data[i]\n",
        "    return X_denoised\n",
        "\n",
        "print(\"Applying noise reduction...\")\n",
        "X_denoised = denoise_time_series(X_imputed)\n",
        "X_test_denoised = denoise_time_series(X_test_imputed)\n",
        "\n",
        "# Enhanced feature extraction\n",
        "def extract_enhanced_features(X_raw):\n",
        "    \"\"\"Extract comprehensive features from NDVI time series\"\"\"\n",
        "    df = pd.DataFrame(X_raw, columns=ndvi_columns)\n",
        "    features = pd.DataFrame()\n",
        "\n",
        "    # Basic statistical features\n",
        "    features['mean'] = df.mean(axis=1)\n",
        "    features['std'] = df.std(axis=1)\n",
        "    features['min'] = df.min(axis=1)\n",
        "    features['max'] = df.max(axis=1)\n",
        "    features['range'] = features['max'] - features['min']\n",
        "    features['median'] = df.median(axis=1)\n",
        "\n",
        "    # Percentile features\n",
        "    features['q25'] = df.quantile(0.25, axis=1)\n",
        "    features['q75'] = df.quantile(0.75, axis=1)\n",
        "    features['iqr'] = features['q75'] - features['q25']\n",
        "    features['skewness'] = df.skew(axis=1)\n",
        "    features['kurtosis'] = df.kurtosis(axis=1)\n",
        "\n",
        "    # Temporal features\n",
        "    features['trend'] = calculate_trend(df)\n",
        "    features['seasonality'] = features['std'] / (features['mean'] + 0.001)\n",
        "\n",
        "    # Vegetation-specific features\n",
        "    features['vegetation_vigor'] = (features['mean'] > 0.3).astype(int)\n",
        "    features['water_indicator'] = (features['mean'] < 0.1).astype(int)\n",
        "    features['high_ndvi_count'] = (df > 0.5).sum(axis=1)\n",
        "    features['low_ndvi_count'] = (df < 0.2).sum(axis=1)\n",
        "\n",
        "    # Peak and valley detection\n",
        "    features['peak_count'] = count_peaks(df.values)\n",
        "    features['valley_count'] = count_valleys(df.values)\n",
        "\n",
        "    # Growing season features (assuming first half vs second half)\n",
        "    mid_point = len(ndvi_columns) // 2\n",
        "    growing_cols = ndvi_columns[:mid_point]\n",
        "    dormant_cols = ndvi_columns[mid_point:]\n",
        "\n",
        "    features['growing_mean'] = df[growing_cols].mean(axis=1)\n",
        "    features['dormant_mean'] = df[dormant_cols].mean(axis=1)\n",
        "    features['seasonal_diff'] = features['growing_mean'] - features['dormant_mean']\n",
        "    features['seasonal_ratio'] = features['growing_mean'] / (features['dormant_mean'] + 0.001)\n",
        "\n",
        "    # Stability features\n",
        "    features['coefficient_variation'] = features['std'] / (features['mean'] + 0.001)\n",
        "    features['amplitude'] = features['max'] - features['min']\n",
        "\n",
        "    return features\n",
        "\n",
        "def calculate_trend(df):\n",
        "    \"\"\"Calculate linear trend for each time series\"\"\"\n",
        "    trends = []\n",
        "    x = np.arange(len(df.columns))\n",
        "    for idx in df.index:\n",
        "        y = df.loc[idx].values\n",
        "        slope = np.polyfit(x, y, 1)[0]\n",
        "        trends.append(slope)\n",
        "    return pd.Series(trends, index=df.index)\n",
        "\n",
        "def count_peaks(data):\n",
        "    \"\"\"Count peaks in time series\"\"\"\n",
        "    peaks = []\n",
        "    for row in data:\n",
        "        count = 0\n",
        "        for i in range(1, len(row)-1):\n",
        "            if row[i] > row[i-1] and row[i] > row[i+1]:\n",
        "                count += 1\n",
        "        peaks.append(count)\n",
        "    return peaks\n",
        "\n",
        "def count_valleys(data):\n",
        "    \"\"\"Count valleys in time series\"\"\"\n",
        "    valleys = []\n",
        "    for row in data:\n",
        "        count = 0\n",
        "        for i in range(1, len(row)-1):\n",
        "            if row[i] < row[i-1] and row[i] < row[i+1]:\n",
        "                count += 1\n",
        "        valleys.append(count)\n",
        "    return valleys\n",
        "\n",
        "# Extract enhanced features\n",
        "print(\"Extracting enhanced features...\")\n",
        "X_features = extract_enhanced_features(X_denoised)\n",
        "X_test_features = extract_enhanced_features(X_test_denoised)\n",
        "\n",
        "print(f\"Number of features: {X_features.shape[1]}\")\n",
        "\n",
        "# Encode target classes\n",
        "class_mapping = {label: i for i, label in enumerate(sorted(y.unique()))}\n",
        "inv_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "y_encoded = y.map(class_mapping).values\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_features)\n",
        "X_test_scaled = scaler.transform(X_test_features)\n",
        "\n",
        "# Optional: Apply PCA for dimensionality reduction if too many features\n",
        "if X_scaled.shape[1] > 50:\n",
        "    print(\"Applying PCA for dimensionality reduction...\")\n",
        "    pca = PCA(n_components=0.95, random_state=42)  # Keep 95% of variance\n",
        "    X_scaled = pca.fit_transform(X_scaled)\n",
        "    X_test_scaled = pca.transform(X_test_scaled)\n",
        "    print(f\"Features reduced to: {X_scaled.shape[1]}\")\n",
        "\n",
        "\"\"\"\n",
        "# Improved ensemble with better hyperparameter tuning\n",
        "def improved_logistic_ensemble(X, y, X_test, n_models=10, random_state=42):\n",
        "    Improved ensemble with hyperparameter tuning and better sampling\n",
        "    skf = StratifiedKFold(n_splits=n_models, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # Different hyperparameter combinations\n",
        "    param_combinations = [\n",
        "        {'C': 0.1, 'solver': 'liblinear', 'multi_class': 'ovr'},\n",
        "        {'C': 1.0, 'solver': 'liblinear', 'multi_class': 'ovr'},\n",
        "        {'C': 10.0, 'solver': 'liblinear', 'multi_class': 'ovr'},\n",
        "        {'C': 0.1, 'solver': 'saga', 'multi_class': 'multinomial'},\n",
        "        {'C': 1.0, 'solver': 'saga', 'multi_class': 'multinomial'},\n",
        "        {'C': 10.0, 'solver': 'saga', 'multi_class': 'multinomial'},\n",
        "        {'C': 0.1, 'solver': 'lbfgs', 'multi_class': 'multinomial'},\n",
        "        {'C': 1.0, 'solver': 'lbfgs', 'multi_class': 'multinomial'},\n",
        "        {'C': 10.0, 'solver': 'lbfgs', 'multi_class': 'multinomial'},\n",
        "\n",
        "    ]\n",
        "\n",
        "    preds_train = []\n",
        "    preds_test = []\n",
        "    model_scores = []\n",
        "    rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"Training model {i+1}/{n_models}\")\n",
        "\n",
        "        # Use different hyperparameters for each model\n",
        "        params = param_combinations[i % len(param_combinations)]\n",
        "\n",
        "        # Feature subsampling (use more features for better performance)\n",
        "        n_features_to_use = max(10, min(X.shape[1], int(0.8 * X.shape[1])))\n",
        "        feature_indices = rng.choice(X.shape[1], size=n_features_to_use, replace=False)\n",
        "\n",
        "        # Train model\n",
        "        clf = LogisticRegression(\n",
        "            random_state=random_state + i,\n",
        "            max_iter=2000,\n",
        "            class_weight='balanced',\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        X_train_subset = X[train_idx][:, feature_indices]\n",
        "        X_val_subset = X[val_idx][:, feature_indices]\n",
        "\n",
        "        clf.fit(X_train_subset, y[train_idx])\n",
        "\n",
        "        # Validate model\n",
        "        val_pred = clf.predict(X_val_subset)\n",
        "        val_score = accuracy_score(y[val_idx], val_pred)\n",
        "        model_scores.append(val_score)\n",
        "        print(f\"Model {i+1} validation accuracy: {val_score:.4f}\")\n",
        "\n",
        "        # Predictions on full datasets\n",
        "       # preds_train.append(clf.predict(X[:, feature_indices]))\n",
        "       # preds_test.append(clf.predict(X_test[:, feature_indices]))\n",
        "\n",
        "        preds_train.append(clf.predict_proba(X[:, feature_indices]))\n",
        "        preds_test.append(clf.predict_proba(X_test[:, feature_indices]))\n",
        "\n",
        "    # Convert to arrays\n",
        "    preds_train = np.array(preds_train)\n",
        "    preds_test = np.array(preds_test)\n",
        "    model_scores = np.array(model_scores)\n",
        "\n",
        "    print(f\"Average model accuracy: {model_scores.mean():.4f} (+/- {model_scores.std()*2:.4f})\")\n",
        "\n",
        "    # Weighted ensemble based on validation scores\n",
        "    weights = model_scores / model_scores.sum()\n",
        "\n",
        "    # Weighted majority vote\n",
        "   # y_train_ensemble = weighted_majority_vote(preds_train, weights)\n",
        "   # y_test_ensemble = weighted_majority_vote(preds_test, weights)\n",
        "\n",
        "    y_train_ensemble = weighted_soft_vote(preds_train, weights)\n",
        "    y_test_ensemble = weighted_soft_vote(preds_test, weights)\n",
        "\n",
        "    return y_train_ensemble, y_test_ensemble, model_scores\n",
        "\"\"\"\n",
        "def generate_param_combinations(n_samples=10, random_state=42):\n",
        "    solvers = ['liblinear']\n",
        "    penalties = {\n",
        "        'liblinear': ['l1', 'l2'],\n",
        "        'saga': ['l1', 'l2', 'elasticnet'],\n",
        "        'lbfgs': ['l2']\n",
        "    }\n",
        "    multi_classes = ['ovr']\n",
        "    Cs = [0.01,0.05, 0.1,0.5, 1.0,5, 10.0,50, 100.0]\n",
        "    l1_ratios = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "\n",
        "    param_list = []\n",
        "\n",
        "    for solver in solvers:\n",
        "        for penalty in penalties[solver]:\n",
        "            for multi_class in multi_classes:\n",
        "                for C in Cs:\n",
        "                    if penalty == 'elasticnet' and solver == 'saga':\n",
        "                        for l1_ratio in l1_ratios:\n",
        "                            param_list.append({\n",
        "                                'solver': solver,\n",
        "                                'penalty': penalty,\n",
        "                                'multi_class': multi_class,\n",
        "                                'C': C,\n",
        "                                'l1_ratio': l1_ratio\n",
        "                            })\n",
        "                    else:\n",
        "                        param_list.append({\n",
        "                            'solver': solver,\n",
        "                            'penalty': penalty,\n",
        "                            'multi_class': multi_class,\n",
        "                            'C': C\n",
        "                        })\n",
        "\n",
        "    rng = random.Random(random_state)\n",
        "    return rng.sample(param_list, k=min(n_samples, len(param_list)))\n",
        "\n",
        "def improved_logistic_ensemble(X, y, X_test, n_models=10, random_state=42):\n",
        "    \"\"\"Improved ensemble with randomized hyperparameter tuning and better sampling\"\"\"\n",
        "    skf = StratifiedKFold(n_splits=n_models, shuffle=True, random_state=random_state)\n",
        "    param_combinations = generate_param_combinations(n_models, random_state)\n",
        "\n",
        "    preds_train = []\n",
        "    preds_test = []\n",
        "    model_scores = []\n",
        "    rng = np.random.default_rng(seed=random_state)\n",
        "\n",
        "    for i, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        print(f\"\\nTraining model {i+1}/{n_models}\")\n",
        "        params = param_combinations[i]\n",
        "\n",
        "        # Feature subsampling (80% features)\n",
        "        n_features_to_use = max(10, min(X.shape[1], int(0.8 * X.shape[1])))\n",
        "        feature_indices = rng.choice(X.shape[1], size=n_features_to_use, replace=False)\n",
        "\n",
        "        clf = LogisticRegression(\n",
        "            random_state=random_state + i,\n",
        "            max_iter=2000,\n",
        "            class_weight='balanced',\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        X_train_subset = X[train_idx][:, feature_indices]\n",
        "        X_val_subset = X[val_idx][:, feature_indices]\n",
        "\n",
        "        try:\n",
        "            clf.fit(X_train_subset, y[train_idx])\n",
        "        except ValueError as e:\n",
        "            print(f\"Model {i+1} skipped due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        val_pred = clf.predict(X_val_subset)\n",
        "        val_score = accuracy_score(y[val_idx], val_pred)\n",
        "        model_scores.append(val_score)\n",
        "        print(f\"Validation accuracy: {val_score:.4f}\")\n",
        "\n",
        "        preds_train.append(clf.predict(X[:, feature_indices]))\n",
        "        preds_test.append(clf.predict(X_test[:, feature_indices]))\n",
        "\n",
        "    if len(preds_train) == 0:\n",
        "        raise ValueError(\"No models successfully trained. Please check your data or parameter space.\")\n",
        "\n",
        "    preds_train = np.array(preds_train)\n",
        "    preds_test = np.array(preds_test)\n",
        "    model_scores = np.array(model_scores)\n",
        "    weights = model_scores / model_scores.sum()\n",
        "\n",
        "    print(f\"\\nAverage model accuracy: {model_scores.mean():.4f} (+/- {model_scores.std()*2:.4f})\")\n",
        "\n",
        "    y_train_ensemble = weighted_majority_vote(preds_train, weights)\n",
        "    y_test_ensemble = weighted_majority_vote(preds_test, weights)\n",
        "\n",
        "    return y_train_ensemble, y_test_ensemble, model_scores\n",
        "\n",
        "def weighted_majority_vote(predictions, weights):\n",
        "    \"\"\"Weighted majority voting\"\"\"\n",
        "    n_samples = predictions.shape[1]\n",
        "    n_classes = len(np.unique(predictions))\n",
        "\n",
        "    ensemble_preds = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Count weighted votes for each class\n",
        "        class_votes = np.zeros(n_classes)\n",
        "        for j, pred in enumerate(predictions[:, i]):\n",
        "            class_votes[pred] += weights[j]\n",
        "\n",
        "        # Choose class with highest weighted vote\n",
        "        ensemble_preds[i] = np.argmax(class_votes)\n",
        "\n",
        "    return ensemble_preds\n",
        "\n",
        "def weighted_soft_vote(prob_predictions, weights):\n",
        "    n_models, n_samples, n_classes = prob_predictions.shape\n",
        "\n",
        "    # Normalize weights to avoid scaling issues\n",
        "    weights = np.array(weights) / np.sum(weights)\n",
        "\n",
        "    # Initialize array to hold final predicted labels\n",
        "    ensemble_preds = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Weighted sum of probabilities for each class\n",
        "        class_probs = np.zeros(n_classes)\n",
        "        for j in range(n_models):\n",
        "            class_probs += weights[j] * prob_predictions[j, i]\n",
        "\n",
        "        # Choose class with highest probability\n",
        "        ensemble_preds[i] = np.argmax(class_probs)\n",
        "\n",
        "    return ensemble_preds\n",
        "\n",
        "# Train improved ensemble\n",
        "print(\"Training improved ensemble...\")\n",
        "y_train_pred, y_test_pred, scores = improved_logistic_ensemble(\n",
        "    X_scaled, y_encoded, X_test_scaled, n_models=7, random_state=42\n",
        ")\n",
        "\n",
        "# Evaluate ensemble performance on training data\n",
        "train_accuracy = accuracy_score(y_encoded, y_train_pred)\n",
        "print(f\"Ensemble training accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "# Convert predictions back to class labels\n",
        "y_test_labels = pd.Series(y_test_pred).map(inv_class_mapping)\n",
        "\n",
        "print(\"Prediction distribution:\")\n",
        "print(pd.Series(y_test_labels).value_counts())\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({'ID': ids_test, 'class': y_test_labels})\n",
        "submission.to_csv('improved_submission.csv', index=False)\n",
        "\n",
        "print(\"\\nSubmission saved as 'improved_submission.csv'\")\n",
        "print(\"First few predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Additional validation using train-test split\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Additional Validation\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Split training data for validation\n",
        "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Train on validation split\n",
        "y_train_pred_val, y_test_pred_val, val_scores = improved_logistic_ensemble(\n",
        "    X_train_val, y_train_val, X_test_val, n_models=7, random_state=42\n",
        ")\n",
        "\n",
        "# Calculate validation accuracy\n",
        "val_accuracy = accuracy_score(y_test_val, y_test_pred_val)\n",
        "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Show classification report\n",
        "y_test_val_labels = [inv_class_mapping[pred] for pred in y_test_val]\n",
        "y_pred_val_labels = [inv_class_mapping[pred] for pred in y_test_pred_val]\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_val_labels, y_pred_val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSEFRxSEZgo3",
        "outputId": "93266447-024b-4948-ce9a-766261e95adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (8000, 30)\n",
            "Test shape: (2845, 29)\n",
            "Class distribution:\n",
            "class\n",
            "forest        6159\n",
            "farm           841\n",
            "impervious     669\n",
            "grass          196\n",
            "water          105\n",
            "orchard         30\n",
            "Name: count, dtype: int64\n",
            "Found 27 NDVI features\n",
            "Missing values in train: 25040\n",
            "Missing values in test: 0\n",
            "Applying noise reduction...\n",
            "Extracting enhanced features...\n",
            "Number of features: 25\n",
            "Training improved ensemble...\n",
            "\n",
            "Training model 1/7\n",
            "Validation accuracy: 0.8110\n",
            "\n",
            "Training model 2/7\n",
            "Validation accuracy: 0.8381\n",
            "\n",
            "Training model 3/7\n",
            "Validation accuracy: 0.8276\n",
            "\n",
            "Training model 4/7\n",
            "Validation accuracy: 0.8311\n",
            "\n",
            "Training model 5/7\n",
            "Validation accuracy: 0.8154\n",
            "\n",
            "Training model 6/7\n",
            "Validation accuracy: 0.8198\n",
            "\n",
            "Training model 7/7\n",
            "Validation accuracy: 0.8187\n",
            "\n",
            "Average model accuracy: 0.8231 (+/- 0.0177)\n",
            "Ensemble training accuracy: 0.8333\n",
            "Prediction distribution:\n",
            "forest        1813\n",
            "impervious     440\n",
            "farm           276\n",
            "grass          155\n",
            "water          106\n",
            "orchard         55\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Submission saved as 'improved_submission.csv'\n",
            "First few predictions:\n",
            "   ID    class\n",
            "0   1   forest\n",
            "1   2   forest\n",
            "2   3   forest\n",
            "3   4   forest\n",
            "4   5   forest\n",
            "5   6     farm\n",
            "6   7  orchard\n",
            "7   8   forest\n",
            "8   9   forest\n",
            "9  10    grass\n",
            "\n",
            "==================================================\n",
            "Additional Validation\n",
            "==================================================\n",
            "\n",
            "Training model 1/7\n",
            "Validation accuracy: 0.8033\n",
            "\n",
            "Training model 2/7\n",
            "Validation accuracy: 0.8393\n",
            "\n",
            "Training model 3/7\n",
            "Validation accuracy: 0.8282\n",
            "\n",
            "Training model 4/7\n",
            "Validation accuracy: 0.8107\n",
            "\n",
            "Training model 5/7\n",
            "Validation accuracy: 0.8129\n",
            "\n",
            "Training model 6/7\n",
            "Validation accuracy: 0.8217\n",
            "\n",
            "Training model 7/7\n",
            "Validation accuracy: 0.8239\n",
            "\n",
            "Average model accuracy: 0.8200 (+/- 0.0224)\n",
            "Validation accuracy: 0.8225\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        farm       0.51      0.29      0.37       168\n",
            "      forest       0.91      0.92      0.92      1232\n",
            "       grass       0.22      0.26      0.24        39\n",
            "  impervious       0.66      0.83      0.74       134\n",
            "     orchard       0.06      0.17      0.09         6\n",
            "       water       0.42      0.62      0.50        21\n",
            "\n",
            "    accuracy                           0.82      1600\n",
            "   macro avg       0.46      0.51      0.47      1600\n",
            "weighted avg       0.82      0.82      0.82      1600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7S9jDdYGaT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}